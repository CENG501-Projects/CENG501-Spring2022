{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIO1AsxUCM8i",
    "outputId": "6a81d660-6ec4-4e5b-c47e-c29e783e610c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/OSOP\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd drive/MyDrive/OSOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEVdNGFwripM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "need_pytorch3d=False\n",
    "try:\n",
    "    import pytorch3d\n",
    "except ModuleNotFoundError:\n",
    "    need_pytorch3d=True\n",
    "if need_pytorch3d:\n",
    "    if torch.__version__.startswith(\"1.11.\") and sys.platform.startswith(\"linux\"):\n",
    "        # We try to install PyTorch3D via a released wheel.\n",
    "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "        version_str=\"\".join([\n",
    "            f\"py3{sys.version_info.minor}_cu\",\n",
    "            torch.version.cuda.replace(\".\",\"\"),\n",
    "            f\"_pyt{pyt_version_str}\"\n",
    "        ])\n",
    "        !pip install fvcore iopath\n",
    "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
    "    else:\n",
    "        # We try to install PyTorch3D from source.\n",
    "        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n",
    "        !tar xzf 1.10.0.tar.gz\n",
    "        os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n",
    "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqqrQ0FADv12",
    "outputId": "7d9cf604-f4aa-49c0-8eb2-7ae6f89ea103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-06 17:32:05--  https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1608 (1.6K) [text/plain]\n",
      "Saving to: ‘plot_image_grid.py.15’\n",
      "\n",
      "plot_image_grid.py. 100%[===================>]   1.57K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2022-07-06 17:32:05 (1.99 MB/s) - ‘plot_image_grid.py.15’ saved [1608/1608]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
    "from plot_image_grid import image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9mH5iVprQdZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_ubyte\n",
    "from PIL import Image\n",
    "\n",
    "# io utils\n",
    "from pytorch3d.io import load_obj, load_ply\n",
    "\n",
    "# datastructures\n",
    "from pytorch3d.structures import Meshes\n",
    "\n",
    "# 3D transformations functions\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "\n",
    "# rendering components\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform, look_at_rotation, \n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d-oREfkrt_Z"
   },
   "source": [
    "If you are running this notebook locally after cloning the PyTorch3D repository, the mesh will already be available. **If using Google Colab, fetch the mesh and save it at the path `data/`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sD5KcLuJr0PL",
    "outputId": "35741a08-3068-43be-b04e-9881b2b82e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-06 16:22:51--  https://dl.fbaipublicfiles.com/pytorch3d/data/teapot/teapot.obj\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152595 (149K) [text/plain]\n",
      "Saving to: ‘data/teapot.obj.9’\n",
      "\n",
      "teapot.obj.9        100%[===================>] 149.02K   239KB/s    in 0.6s    \n",
      "\n",
      "2022-07-06 16:22:53 (239 KB/s) - ‘data/teapot.obj.9’ saved [152595/152595]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget -P data https://dl.fbaipublicfiles.com/pytorch3d/data/teapot/teapot.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eYDX30cCqC-",
    "outputId": "5bebab4d-88d8-4516-95fd-3c78cc369dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/                  plot_image_grid.py.13  plot_image_grid.py.6\n",
      "\u001b[01;34mmodels_cad\u001b[0m/            plot_image_grid.py.14  plot_image_grid.py.7\n",
      "plot_image_grid.py     plot_image_grid.py.15  plot_image_grid.py.8\n",
      "plot_image_grid.py.1   plot_image_grid.py.2   plot_image_grid.py.9\n",
      "plot_image_grid.py.10  plot_image_grid.py.3   \u001b[01;34m__pycache__\u001b[0m/\n",
      "plot_image_grid.py.11  plot_image_grid.py.4   t-less_download.py\n",
      "plot_image_grid.py.12  plot_image_grid.py.5\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWiPKnEIrQdd"
   },
   "outputs": [],
   "source": [
    "# Set the cuda device \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the obj and ignore the textures and materials.\n",
    "#verts, faces_idx, _ = load_obj(\"./data/teapot.obj\")\n",
    "#faces = faces_idx.verts_idx\n",
    "\n",
    "verts, faces = load_ply(\"models_cad/obj_01.ply\")\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = TexturesVertex(verts_features=verts_rgb.to(device))\n",
    "\n",
    "# Create a Meshes object for the teapot. Here we have only one mesh in the batch.\n",
    "teapot_mesh = Meshes(\n",
    "    verts=[verts.to(device)],   \n",
    "    faces=[faces.to(device)], \n",
    "    textures=textures\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPlby75GrQdj"
   },
   "outputs": [],
   "source": [
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(device=device, cameras=cameras, lights=lights)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OAkUOqGH7zI"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "batch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "id": "SI2YunTOjgm6",
    "outputId": "538cc3b7-edac-4161-bb28-caf5ec307636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch_count: 0 0 - 100\n",
      "Saved batch_count: 1 100 - 200\n",
      "Saved batch_count: 2 200 - 300\n",
      "Saved batch_count: 3 300 - 400\n",
      "Saved batch_count: 4 400 - 500\n",
      "Saved batch_count: 5 500 - 600\n",
      "Saved batch_count: 6 600 - 700\n",
      "Saved batch_count: 7 700 - 800\n",
      "Saved batch_count: 8 800 - 900\n",
      "Saved batch_count: 9 900 - 1000\n",
      "Saved batch_count: 10 1000 - 1100\n",
      "Saved batch_count: 11 1100 - 1200\n",
      "Saved batch_count: 12 1200 - 1300\n",
      "Saved batch_count: 13 1300 - 1400\n",
      "Saved batch_count: 14 1400 - 1500\n",
      "Saved batch_count: 15 1500 - 1600\n",
      "Saved batch_count: 16 1600 - 1700\n",
      "Saved batch_count: 17 1700 - 1800\n",
      "Saved batch_count: 18 1800 - 1900\n",
      "Saved batch_count: 19 1900 - 2000\n",
      "Saved batch_count: 20 2000 - 2100\n",
      "Saved batch_count: 21 2100 - 2200\n",
      "Saved batch_count: 22 2200 - 2300\n",
      "Saved batch_count: 23 2300 - 2400\n",
      "Saved batch_count: 24 2400 - 2500\n",
      "Saved batch_count: 25 2500 - 2600\n",
      "Saved batch_count: 26 2600 - 2700\n",
      "Saved batch_count: 27 2700 - 2800\n",
      "Saved batch_count: 28 2800 - 2900\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-787c07dff2d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m# view the camera from the same distance and specify dist=2.7 as a float,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# and then specify elevation and azimuth angles for each viewpoint as tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlook_at_view_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mazim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mazim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mcameras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFoVPerspectiveCameras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch3d/renderer/cameras.py\u001b[0m in \u001b[0;36mlook_at_view_transform\u001b[0;34m(dist, elev, azim, degrees, eye, at, up, device)\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m         broadcasted_args = convert_to_tensors_and_broadcast(\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mazim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m         )\n\u001b[1;32m   1704\u001b[0m         \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mazim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcasted_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch3d/renderer/utils.py\u001b[0m in \u001b[0;36mconvert_to_tensors_and_broadcast\u001b[0;34m(dtype, device, *args)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Got non-broadcastable sizes %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;31m# Expand broadcast dim and keep non broadcast dims the same size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Got non-broadcastable sizes [1, 0, 0, 1, 1]"
     ]
    }
   ],
   "source": [
    "total_batch_size = 2900\n",
    "total_elev = torch.linspace(0, 180, total_batch_size)\n",
    "total_azim = torch.linspace(-180, 180, total_batch_size)\n",
    "\n",
    "batch_size = 100\n",
    "total_count = 0\n",
    "\n",
    "while total_count <= total_batch_size:\n",
    "  meshes = teapot_mesh.extend(batch_size)\n",
    "\n",
    "  # Get a batch of viewing angles. \n",
    "  elev = total_elev[batch_count*batch_size:(batch_count+1)*batch_size]\n",
    "  azim = total_elev[batch_count*batch_size:(batch_count+1)*batch_size]\n",
    "\n",
    "\n",
    "  # All the cameras helper methods support mixed type inputs and broadcasting. So we can \n",
    "  # view the camera from the same distance and specify dist=100 as a float,\n",
    "  # and then specify elevation and azimuth angles for each viewpoint as tensors. \n",
    "  R, T = look_at_view_transform(dist=100, elev=elev, azim=azim)\n",
    "  cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "  # Move the light back in front of the cow which is facing the -z direction.\n",
    "  lights.location = torch.tensor([[0.0, 0.0, -3.0]], device=device)\n",
    "\n",
    "  images = phong_renderer(meshes, cameras=cameras, lights=lights)\n",
    "  print(\"Saved batch_count: {} {} - {}\".format(batch_count, batch_count*batch_size, (batch_count+1)*batch_size))\n",
    "  batch_count += 1\n",
    "  total_count += batch_size\n",
    "\n",
    "  for image in images:\n",
    "    img = image.cpu().numpy().squeeze()\n",
    "    img = img[:, :, :3]\n",
    "    img = np.array(img*255, dtype=np.uint8)\n",
    "    im = Image.fromarray(img)\n",
    "    im.save(\"Export/image_\" + str(count) + \".png\")\n",
    "    count += 1\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anp_metadata": {
   "path": "fbsource/fbcode/vision/fair/pytorch3d/docs/tutorials/camera_position_optimization_with_differentiable_rendering.ipynb"
  },
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of camera_position_optimization_with_differentiable_rendering.ipynb",
   "provenance": []
  },
  "disseminate_notebook_info": {
   "backup_notebook_id": "1062179640844868"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
