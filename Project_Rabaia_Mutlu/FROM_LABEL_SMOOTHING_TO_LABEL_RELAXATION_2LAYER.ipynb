{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FROM_LABEL_SMOOTHING_TO_LABEL_RELAXATION_2LAYER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X4X2hmaEmip"
      },
      "source": [
        "# CENG501 - PROJECT\n",
        "# FROM LABEL SMOOTHING TO LABEL RELAXATION\n",
        "Simple 2-Layer architecture\n",
        "Lienen, J.; HÃ¼llermeier, E. 2021. From Label Smoothing to Label Relaxation. Proceedings of the AAAI Conference on Artificial Intelligence, 35(10), 8583-8591. Retrieved from https://ojs.aaai.org/index.php/AAAI/article/view/17041\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRaKrjO5JSu-"
      },
      "source": [
        "## 1 Import the Modules\n",
        "\n",
        "Here, we import some libraries that we will use throughout the implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4L5nogMKyNx"
      },
      "source": [
        "import matplotlib.pyplot as plt # For plotting\n",
        "import numpy as np              # NumPy, for working with arrays/tensors \n",
        "import time                     # For measuring time\n",
        "\n",
        "# PyTorch libraries:\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BXVQOGRI4Sc"
      },
      "source": [
        "### 1.1 Enable GPU\n",
        "\n",
        "From \"Edit -> Notebook Settings -> Hardware accelerator\" select GPU. With the following we will specify to PyTorch that we want to use the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97DJEyArJLcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386e259b-a127-4d2d-8ad1-8785aa710fa0"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print(\"Cuda (GPU support) is available and enabled!\")\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  print(\"Cuda (GPU support) is not available :(\")\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda (GPU support) is available and enabled!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMmi17e-JX7o"
      },
      "source": [
        "## 2 The Dataset\n",
        "\n",
        "We will use Keras datasets to download the MNIST dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlXFbhR8Aq0m"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "(x_train_ini, y_train_ini), (x_test_ini, y_test_ini) = datasets.mnist.load_data()\n",
        "\n",
        "\n",
        "x_train_ini = np.expand_dims(x_train_ini, axis=-1)\n",
        "x_test_ini = np.expand_dims(x_test_ini, axis=-1)\n",
        "\n",
        "\n",
        "x_test_ratio = 1/7\n",
        "x_val_ratio = 1/6\n",
        "\n",
        "mnist_x = np.concatenate((x_train_ini,  x_test_ini))\n",
        "mnist_y = np.concatenate((y_train_ini,  y_test_ini))\n",
        "\n",
        "x_train_val, x_test, y_train_val, y_test = train_test_split(mnist_x, mnist_y, test_size=x_test_ratio, random_state=0, shuffle=True)\n",
        "\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=x_val_ratio, random_state=0, shuffle=False)\n",
        "\n",
        "#### preparing data for experiments ####################\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_train = x_train.reshape(-1, 28*28)      \n",
        "pixel_mean_train = np.mean(x_train, axis=0)\n",
        "x_train -= pixel_mean_train # substracting mean\n",
        "\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_test = x_test.reshape(-1, 28*28)      \n",
        "pixel_mean_test = np.mean(x_test, axis=0)\n",
        "x_test -= pixel_mean_test # substracting mean\n",
        "\n",
        "x_val = x_val.astype('float32') / 255.\n",
        "x_val = x_val.reshape(-1, 28*28)      \n",
        "pixel_mean_val = np.mean(x_val, axis=0)\n",
        "x_val -= pixel_mean_val # substracting mean\n",
        "\n",
        "# combining training and validation sets to be used in the final training\n",
        "\n",
        "x_train_val = x_train_val.astype('float32') / 255.\n",
        "x_train_val = x_train_val.reshape(-1, 28*28)      \n",
        "pixel_mean_trainval = np.mean(x_train_val, axis=0)\n",
        "x_train_val -= pixel_mean_trainval # substracting mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Label Relaxation Loss Definition"
      ],
      "metadata": {
        "id": "nrk2wmNM-8o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Label_Relaxation(nn.Module):\n",
        "\n",
        "  def __init__(self, alpha=0.5, num_classes=10, toll = 0.05 ):\n",
        "      super(Label_Relaxation, self).__init__()\n",
        "\n",
        "      # define parameters:\n",
        "\n",
        "      self.alpha = alpha\n",
        "      self.num_classes= num_classes\n",
        "      self.toll = toll\n",
        "\n",
        "  def forward(self, y_hat, y): # scores, targets\n",
        "\n",
        "    y = F.one_hot(y, self.num_classes ) # one-hot encoding of the targets.\n",
        "\n",
        "    #Note: y_hat represents the scores without softmax. (to be applied in the loss function)\n",
        "    y_hat =  y_hat.softmax(-1) # convert scores to prop. with softmax.\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      pr1 = torch.ones_like(y) - self.alpha\n",
        "      pr2 = self.alpha * y_hat / torch.unsqueeze(torch.sum((torch.ones_like(y) - y) * y_hat, dim = -1), dim = -1)  \n",
        "      pr = torch.where( self.toll < y , pr1, pr2)\n",
        "\n",
        "                      \n",
        "    # Kullback-Leibler divergence\n",
        "    D_kl = torch.sum(F.kl_div(y_hat.log(), pr, size_average=None, reduce=None, reduction='none', log_target=False), dim =-1)      \n",
        "\n",
        "    y_hat = torch.sum(y*y_hat, dim =-1)\n",
        "\n",
        "    out_loss = torch.mean( torch.where( torch.gt(y_hat, 1.0 - self.alpha), torch.zeros_like(D_kl), D_kl ) )\n",
        "\n",
        "    return  out_loss"
      ],
      "metadata": {
        "id": "oHrYdGh21juA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUmdAA1QJvRx"
      },
      "source": [
        "## 4 Define Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJq7aT_wh9SK"
      },
      "source": [
        "### 4.1 Model Definition\n",
        "\n",
        "Our simple dense model is composed of two hidden layer with ReLu activation. It is inspired from Pytorch examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n13wrSZTiDEp"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TwoLayerNet(nn.Module):\n",
        "    def __init__(self, D, H, C):\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        torch.manual_seed(501)        \n",
        "        \n",
        "        self.fc1 = nn.Linear(D, H)\n",
        "        self.fc2 = nn.Linear(H, C)\n",
        "        self.Relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "      \n",
        "      x = self.fc1(x)\n",
        "      \n",
        "      x = self.Relu(x)\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r8pT7Y9FmDH"
      },
      "source": [
        "### 4.2 Define criterion and the Optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEn3vti5Fmhp"
      },
      "source": [
        "\n",
        "D = 28*28 # dimensionality\n",
        "C = 10 # num of classes\n",
        "H = 1024 # number of hidden neurons\n",
        "\n",
        "\n",
        "\" for Cross_Entropy: criterion = nn.CrossEntropyLoss() , where alpha=0 \"\n",
        "\" for Label_Smoothing: criterion = nn.CrossEntropyLoss(label_smoothing = alpha) , where alpha= 0.0 ~ 1.0 \"\n",
        "\" for Label_Relaxation: criterion = Label_Relaxation(alpha = 0.0 ~ 1.0, num_classes = C) \"\n",
        "# create an instance\n",
        "model = TwoLayerNet(D, H, C)\n",
        "criterion = Label_Relaxation(alpha = 0.1, num_classes = C)\n",
        "optimizer = optim.SGD(model.parameters(), lr=5e-2, weight_decay=1e-5,  momentum = 0.9) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvpiVF5uiBE9"
      },
      "source": [
        "### 4.3 Define Trainer with batch function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Flgr2C6Xb2s"
      },
      "source": [
        "def sample_batch(X, y, batch_size): # sampling (creating batches)\n",
        "  \"\"\"Get a random batch of size batch_size from (X, y).\"\"\"\n",
        "  batch_indices = np.random.choice(range(X.shape[0]), size=batch_size)\n",
        "  X_batch = X[batch_indices]\n",
        "  y_batch = y[batch_indices]\n",
        "\n",
        "  return X_batch, y_batch\n",
        "\n",
        "def train(model, criterion, optimizer, epochs, x, y , batch_size, verbose=True): # defining trainer\n",
        "  \n",
        "  loss_history = []\n",
        "  num_train, dim1 = x.shape\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    for it in range(int(num_train/batch_size)): \n",
        "        \n",
        "      # Get a batch of samples\n",
        "      inputs, labels = sample_batch(x, y, batch_size)\n",
        "      \n",
        "      inputs = torch.Tensor(inputs)\n",
        "     \n",
        "      labels = torch.LongTensor(labels)\n",
        "      \n",
        "      labels = labels.reshape(batch_size).to(device)\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # zero the gradients as PyTorch accumulates them\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Obtain the scores\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = criterion(outputs.to(device), labels)\n",
        "\n",
        "      # Backpropagate\n",
        "      loss.backward()\n",
        "\n",
        "      # Update the weights\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_history.append(loss.item())\n",
        "    \n",
        "    if verbose: print(f'Epoch {epoch} / {epochs}:  loss of last iteration {np.sum(loss_history[-1])}')\n",
        "    \n",
        "  return loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr7NvUrWW9ax"
      },
      "source": [
        "### 5 ECE Loss Definition 'Expected Calibration Error (ECE)'\n",
        "The Expected Calibration Error (ECE) loss function is copied from https://github.com/gpleiss/temperature_scaling/blob/master/temperature_scaling.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmBmxHGoVE_F"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class ECELoss(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_bins=15 ):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQw2uB8kXFu-"
      },
      "source": [
        "### 5.1 Creating a training instance\n",
        "For temperature scaling, to find the optimal temperature, we fix the hyperparameters with initial values and tune the model for T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_fNyLKWV5kg"
      },
      "source": [
        "# fixing hyperparameters e.g.\n",
        "lr = 0.05\n",
        "alpha = 0.2\n",
        "epochs = 25\n",
        "\n",
        "\n",
        "D = 28*28 # dimensionality\n",
        "C = 10 # num of classes\n",
        "H = 1024 # number of hidden neurons\n",
        "\n",
        "model = TwoLayerNet(D, H, C)\n",
        "\n",
        "criterion = Label_Relaxation(alpha = alpha, num_classes =C)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-5,  momentum = 0.9) \n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "loss_history = train(model, criterion, optimizer, epochs, x_train, y_train, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Tuning model for Temperature Scaling (temperature calibration)"
      ],
      "metadata": {
        "id": "SsqbkLSXLm0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score # import  sklearn.metrics to calculate the accuracy scores\n",
        "\n",
        "Temp_Scaling = True \n",
        "\n",
        "if Temp_Scaling:\n",
        "  T = [0.25, 0.5, 0.75, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.5, 3.0]\n",
        "else: \n",
        "  T = [1.] # in case of no temperature scaling just fix T to 1.\n",
        "\n",
        "\n",
        "results_a = {}\n",
        "best_val_a = -1   \n",
        "best_lr_a = None  \n",
        "best_t_a = None  \n",
        "\n",
        "\n",
        "results_e = {}\n",
        "best_val_e = 10**6   \n",
        "best_lr_e = None  \n",
        "best_t_e = None  \n",
        "\n",
        "num_val, dim1 = x_val.shape\n",
        "\n",
        "for temperature in T:\n",
        "    \n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      inputs = x_val\n",
        "      labels = y_val\n",
        "        \n",
        "      inputs = torch.Tensor(inputs)\n",
        "      labels = torch.LongTensor(labels)\n",
        "      \n",
        "\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      \n",
        "        \n",
        "      outputs = model(inputs)\n",
        "\n",
        "      outputs = outputs/temperature\n",
        "\n",
        "      # Calculate validation ECE error\n",
        "      cal_model = ECELoss(n_bins = 15)\n",
        "      ece = cal_model(outputs, labels)\n",
        "      val_error = float(ece[0])\n",
        "        \n",
        "    \n",
        "      # Calculate  validation accuracies\n",
        "      _, predicted = torch.max(outputs.data, 1)  \n",
        "      val_accuracy = accuracy_score(torch.Tensor.cpu(labels), torch.Tensor.cpu(predicted))\n",
        "      \n",
        "\n",
        "      print(f\"learning rate={lr} and temperature={temperature} provided val_accuracy={val_accuracy:.5f}\")\n",
        "      print(f\"learning rate={lr} and temperature={temperature} provided val_error={val_error:.5f}\")\n",
        "    \n",
        "     # Save the results for accuracy\n",
        "      results_a[(lr,temperature)] = (val_accuracy)\n",
        "      if best_val_a < val_accuracy:\n",
        "          best_lr_a = lr\n",
        "          best_temperature_a = temperature\n",
        "          best_val_a = val_accuracy\n",
        "          best_model_a = model\n",
        "\n",
        "      # Save the results for ECE\n",
        "      results_e[(lr,temperature)] = (val_error)\n",
        "      if best_val_e > val_error:\n",
        "          best_lr_e = lr\n",
        "          best_temperature_e = temperature\n",
        "          best_val_e = val_error\n",
        "          best_model_e = model\n",
        "    \n",
        "print(f'\\nbest validation accuracy achieved during cross-validation: {best_val_a:.3f} with params temperature= {best_temperature_a} and lr={best_lr_a}')\n",
        "print(f'\\nbest validation error achieved during cross-validation: {best_val_e:.3f} with params temperature= {best_temperature_e} and lr={best_lr_e}')"
      ],
      "metadata": {
        "id": "eLfr887jLrVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KD1t5l6WpzE"
      },
      "source": [
        "### 5.3 Tuning other parameters with fixed temperature scale\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOALtgdcWsHj"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "epochs = 25\n",
        "temperature_a = best_temperature_a # opt. temperature for accuracy\n",
        "temperature_e = best_temperature_e # opt. temperature for error\n",
        "\n",
        "\n",
        "lr_range = [0.05, 0.05*(0.3), 0.05*(0.3**2),  0.05*(0.3**3) ]\n",
        "\n",
        "alpha_range = [0.01, 0.025, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "\n",
        "# model definition\n",
        "D = 28*28 # dimensionality\n",
        "C = 10 # num of classes\n",
        "H = 1024 # number of hidden neurons\n",
        "\n",
        "model = TwoLayerNet(D, H, C)\n",
        "\n",
        "# creating storage for results\n",
        "results_a = {}\n",
        "best_val_a = -1   \n",
        "best_lr_a = None  \n",
        "best_t_a = None  \n",
        "\n",
        "\n",
        "results_e = {}\n",
        "best_val_e = 10**6   \n",
        "best_lr_e = None  \n",
        "best_t_e = None  \n",
        "\n",
        "# validation set properities\n",
        "num_val, dim1 = x_val.shape\n",
        "\n",
        "\n",
        "\n",
        "for lr in lr_range:\n",
        "\n",
        "  for alpha in alpha_range:\n",
        "    \n",
        "\n",
        "      inputs = x_val\n",
        "      labels = y_val\n",
        "        \n",
        "      inputs = torch.Tensor(inputs)\n",
        "      labels = torch.LongTensor(labels)\n",
        "      #labels = labels.reshape(batch_size).to(device)\n",
        "\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      ##################\n",
        "      #criterion = nn.CrossEntropyLoss()\n",
        "      criterion = Label_Relaxation(alpha = alpha, num_classes =10)\n",
        "      optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-5,  momentum = 0.9) \n",
        "\n",
        "      model = model.to(device)\n",
        "\n",
        "      loss_history = train(model, criterion, optimizer, epochs, x_train, y_train, batch_size)\n",
        "\n",
        "      ################\n",
        "        \n",
        "      outputs = model(inputs)\n",
        "\n",
        "      outputs_a = outputs/temperature_a\n",
        "      outputs_e = outputs/temperature_e\n",
        "\n",
        "\n",
        "      # Calculate validation ECE error\n",
        "      cal_model = ECELoss(n_bins = 15)\n",
        "      ece = cal_model(outputs_e, labels)\n",
        "      val_error = float(ece[0])\n",
        "        \n",
        "    \n",
        "      # Calculate  validation accuracies\n",
        "      _, predicted = torch.max(outputs_a.data, 1)  \n",
        "      val_accuracy = accuracy_score(torch.Tensor.cpu(labels), torch.Tensor.cpu(predicted))\n",
        "      \n",
        "\n",
        "      # print(f\"learning rate={lr} and temperature={temperature} provided val_accuracy={val_accuracy:.5f}\")\n",
        "      # print(f\"learning rate={lr} and temperature={temperature} provided val_error={val_error:.5f}\")\n",
        "    \n",
        "     # Save the results for accuracy\n",
        "      results_a[(lr,temperature)] = (val_accuracy)\n",
        "      if best_val_a < val_accuracy:\n",
        "          best_lr_a = lr\n",
        "          best_alpha_a = alpha\n",
        "          best_val_a = val_accuracy\n",
        "          best_model_a = model\n",
        "\n",
        "      # Save the results for ECE\n",
        "      results_e[(lr,temperature)] = (val_error)\n",
        "      if best_val_e > val_error:\n",
        "          best_lr_e = lr\n",
        "          best_alpha_e = alpha\n",
        "          best_val_e = val_error\n",
        "          best_model_e = model\n",
        "    \n",
        "print(f'\\nbest validation accuracy achieved during cross-validation: {best_val_a:.3f} with params alpha= {best_alpha_a} and lr={best_lr_a}')\n",
        "print(f'\\nbest validation error achieved during cross-validation: {best_val_e:.3f} with params alpha= {best_alpha_e} and lr={best_lr_e}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRPl6GSAW1mz"
      },
      "source": [
        "### 6 Final training of the model for accuracy\n",
        "\n",
        "*Disclaimer: This code piece is taken from PyTorch examples.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM1Fin7mW2B0"
      },
      "source": [
        "\n",
        "# training for the best hyperparameters  of  Accuracy type\n",
        "\n",
        "lr = best_lr_a\n",
        "alpha = best_alpha_a\n",
        "epochs = 25\n",
        "# temperature_a = best_temperature_a # opt. temperature for accuracy\n",
        "# temperature_e = best_temperature_e # opt. temperature for error\n",
        "\n",
        "\n",
        "D = 28*28 # dimensionality\n",
        "C = 10 # num of classes\n",
        "H = 1024 # number of hidden neurons\n",
        "\n",
        "model = TwoLayerNet(D, H, C)\n",
        "\n",
        "\n",
        "criterion = Label_Relaxation(alpha = alpha, num_classes =C)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-5,  momentum = 0.9) \n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "loss_history = train(model, criterion, optimizer, epochs, x_train_val, y_train_val, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Calculating test accuracy/error with Accuracy"
      ],
      "metadata": {
        "id": "UcD8Nq_7AcQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = x_test\n",
        "labels = y_test\n",
        "        \n",
        "inputs = torch.Tensor(inputs)\n",
        "labels = torch.LongTensor(labels)\n",
        "\n",
        "inputs = inputs.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = model(inputs)\n",
        "\n",
        "outputs_a = outputs/temperature_a\n",
        "outputs_e = outputs/temperature_a\n",
        "\n",
        "\n",
        "# Calculate validation ECE error\n",
        "cal_model = ECELoss(n_bins = 15)\n",
        "ece = cal_model(outputs_e, labels)\n",
        "test_error = float(ece[0])\n",
        "        \n",
        "# Calculate  validation accuracies\n",
        "_, predicted = torch.max(outputs_a.data, 1)  \n",
        "test_accuracy = accuracy_score(torch.Tensor.cpu(labels), torch.Tensor.cpu(predicted))\n",
        "print(f'\\nbest test accuracy achieved using Accuracy hyperparameters : {test_accuracy:.3f} with params alpha= {best_alpha_a} and lr={best_lr_a}')\n",
        "print(f'\\nbest test error achieved using Accuracy hyperparameters : {test_error:.3f} with params alpha= {best_alpha_a} and lr={best_lr_a}')"
      ],
      "metadata": {
        "id": "suuVjECHAbGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7 Final training of the model for ECE"
      ],
      "metadata": {
        "id": "mA844iw1E6Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training for the best hyperparameters  of  ECE\n",
        "\n",
        "lr = best_lr_e\n",
        "alpha = best_alpha_e\n",
        "epochs = 25\n",
        "\n",
        "D = 28*28 # dimensionality\n",
        "C = 10 # num of classes\n",
        "H = 1024 # number of hidden neurons\n",
        "\n",
        "model = TwoLayerNet(D, H, C)\n",
        "\n",
        "\n",
        "criterion = Label_Relaxation(alpha = alpha, num_classes =C)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-5,  momentum = 0.9) \n",
        "\n",
        "model = model.to(device)\n",
        "loss_history = train(model, criterion, optimizer, epochs, x_train_val, y_train_val, batch_size)"
      ],
      "metadata": {
        "id": "YatMzE4gFHZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Calculating test accuracy/error with ECE"
      ],
      "metadata": {
        "id": "5PLGqYgPGOb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = x_test\n",
        "labels = y_test\n",
        "        \n",
        "inputs = torch.Tensor(inputs)\n",
        "labels = torch.LongTensor(labels)\n",
        "\n",
        "inputs = inputs.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = model(inputs)\n",
        "\n",
        "outputs_a = outputs/temperature_e\n",
        "outputs_e = outputs/temperature_e\n",
        "\n",
        "\n",
        "# Calculate validation ECE error\n",
        "cal_model = ECELoss(n_bins = 15)\n",
        "ece = cal_model(outputs_e, labels)\n",
        "test_error = float(ece[0])\n",
        "        \n",
        "# Calculate  validation accuracies\n",
        "_, predicted = torch.max(outputs_a.data, 1)  \n",
        "test_accuracy = accuracy_score(torch.Tensor.cpu(labels), torch.Tensor.cpu(predicted))\n",
        "print(f'\\nbest test accuracy achieved using ECE hyperparameters : {test_accuracy:.3f} with params alpha= {best_alpha_e} and lr={best_lr_e}')\n",
        "print(f'\\nbest test error achieved using ECE hyperparameters : {test_error:.3f} with params alpha= {best_alpha_e} and lr={best_lr_e}')"
      ],
      "metadata": {
        "id": "Ge7q2q3GGu_q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}